{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-transfer-codebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Codebook Style Transfer\n",
        "\n",
        "In this notebook we describe and demostrate our simplest method that was used as a baseline model. Recall that the encoder first transforms the input audio into latent vectors $h_s$ and then the vectors are quantized to the closest codebook vector $e_s$. The intuition behind this first method is to quantize the latent vectors from $x$ (content) using only the unique vectors from $y$ (style).\n",
        "\n",
        "First, we obtain the latent vectors from $x$ and $y$ ( $h_x$ and $h_y$ ). For $y$ we quantize the vectors using the full codebook to obtain $e_y$ and for $x$ we quantize the vectors using only the unique vectors from $e_y$. "
      ],
      "metadata": {
        "id": "43CgTMdpjJ2-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taDHgk1WCC_C",
        "outputId": "d51a47a4-4def-4943-8f48-101a558409a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import jukebox\n",
        "from torch import float64\n",
        "import torch as t\n",
        "import torch\n",
        "import librosa\n",
        "import os\n",
        "from IPython.display import Audio\n",
        "from jukebox.make_models import make_vqvae, make_prior, MODELS, make_model\n",
        "from jukebox.hparams import Hyperparams, setup_hparams\n",
        "from jukebox.sample import sample_single_window, _sample, \\\n",
        "                           sample_partial_window, upsample\n",
        "from jukebox.utils.dist_utils import setup_dist_from_mpi\n",
        "from jukebox.utils.torch_utils import empty_cache\n",
        "rank, local_rank, device = setup_dist_from_mpi()\n",
        "\n",
        "import scipy\n",
        "import torch\n",
        "import numpy as np\n",
        "import lucent\n",
        "from lucent.optvis.render import hook_model\n",
        "from lucent.modelzoo.util import get_model_layers\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io.wavfile import read, write\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65aR2OZxmfzq",
        "outputId": "ecb72a03-3c9e-4efa-bdac-fa214d45d37e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = \"1b_lyrics\" # or \"1b_lyrics\"     \n",
        "hps = Hyperparams()\n",
        "hps.sr = 44100\n",
        "hps.n_samples = 3 if model=='5b_lyrics' else 8\n",
        "hps.name = 'samples'\n",
        "chunk_size = 16 if model==\"5b_lyrics\" else 32\n",
        "max_batch_size = 3 if model==\"5b_lyrics\" else 16\n",
        "hps.levels = 3\n",
        "hps.hop_fraction = [.5,.5,.125]\n",
        "\n",
        "vqvae, *priors = MODELS[model]\n",
        "vqvae = make_vqvae(setup_hparams(vqvae, dict(sample_length = 1048576)), device)\n",
        "top_prior = make_prior(setup_hparams(priors[-1], dict()), vqvae, device)\n",
        "vqvae = vqvae.eval()\n",
        "\n",
        "f_start = 100\n",
        "f_end = 4000\n",
        "num_seconds = 10\n",
        "sample_rate = 44100\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from azure\n",
            "Running  wget -O /root/.cache/jukebox/models/5b/vqvae.pth.tar https://openaipublic.azureedge.net/jukebox/models/5b/vqvae.pth.tar\n",
            "Restored from /root/.cache/jukebox/models/5b/vqvae.pth.tar\n",
            "0: Loading vqvae in eval mode\n",
            "Creating cond. autoregress with prior bins [79, 2048], \n",
            "dims [384, 6144], \n",
            "shift [ 0 79]\n",
            "input shape 6528\n",
            "input bins 2127\n",
            "Self copy is False\n",
            "Loading artist IDs from /usr/local/lib/python3.7/dist-packages/jukebox/data/ids/v3_artist_ids.txt\n",
            "Loading artist IDs from /usr/local/lib/python3.7/dist-packages/jukebox/data/ids/v3_genre_ids.txt\n",
            "Level:2, Cond downsample:None, Raw to tokens:128, Sample length:786432\n",
            "Downloading from azure\n",
            "Running  wget -O /root/.cache/jukebox/models/1b_lyrics/prior_level_2.pth.tar https://openaipublic.azureedge.net/jukebox/models/1b_lyrics/prior_level_2.pth.tar\n",
            "Restored from /root/.cache/jukebox/models/1b_lyrics/prior_level_2.pth.tar\n",
            "0: Loading prior in eval mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_wave(instrument, note):    \n",
        "  file_name = \"data/notes/{}-{}.wav\".format(instrument, note)\n",
        "  a = read(file_name)\n",
        "  wave = np.array(a[1],dtype='int16')\n",
        "  return a[0], wave\n",
        "\n",
        "def encode(vqvae, wave):\n",
        "  x = torch.from_numpy(wave).unsqueeze(0).unsqueeze(2).cuda()\n",
        "  x_in = vqvae.preprocess(x)\n",
        "  xs = []\n",
        "  for level in range(vqvae.levels):\n",
        "      encoder = vqvae.encoders[level]\n",
        "      x_out = encoder(x_in)\n",
        "      xs.append(x_out[-1])\n",
        "\n",
        "  return xs\n",
        "\n",
        "def decode(vqvae, xs, quant=True):\n",
        "\n",
        "  if quant:\n",
        "    zs, xs_quantised, _, _ = vqvae.bottleneck(xs)\n",
        "  else:\n",
        "    xs_quantised = xs \n",
        "\n",
        "  x_outs = []\n",
        "  x_outs_nonquantised = []\n",
        "  for level in range(vqvae.levels):\n",
        "      decoder = vqvae.decoders[level]    \n",
        "      x_out = decoder(xs_quantised[level:level+1], all_levels=False)\n",
        "      x_outs.append(x_out)\n",
        "      x_outs[level] = vqvae.postprocess(x_outs[level])\n",
        "\n",
        "  return x_outs\n",
        "\n",
        "\n",
        "def make_continous(discrete, continous):\n",
        "  a, num_discrete = discrete.shape\n",
        "  _, num_continous = continous.shape\n",
        "  ref = torch.empty((a,num_discrete), dtype =torch.float)\n",
        "\n",
        "  for d in range(num_discrete):\n",
        "    smallest_distance = 1000000000000\n",
        "    nn = 0 # index of nn\n",
        "    current_discrete = discrete[:,d]\n",
        "\n",
        "    for c in range(num_continous):\n",
        "      current_cont = continous[:,c]\n",
        "      delta = current_discrete - current_cont \n",
        "      distance = torch.sqrt(torch.pow(delta, 2).sum(dim=0))\n",
        "\n",
        "      if distance < smallest_distance:\n",
        "        smallest_distance = distance\n",
        "        nn = c\n",
        "\n",
        "    ref[:,d] = continous[:,nn]\n",
        "\n",
        "  return ref.cuda()\n",
        "\n",
        "\n",
        "def transfer_codebook(vqvae, hx, hy):\n",
        "  # Content\n",
        "  num_layers = len(hx)\n",
        "\n",
        "  # Style\n",
        "  zs, y_quantised, _, _ = vqvae.bottleneck(hy)\n",
        "\n",
        "  x_transfered = []\n",
        "\n",
        "  for layer in range(num_layers):\n",
        "    query = hx[layer][0]\n",
        "\n",
        "    ref_discrete = torch.unique(torch.floor(y_quantised[layer][0]), dim=1)\n",
        "    ref = make_continous(ref_discrete, y_quantised[layer][0])\n",
        "\n",
        "    len_emb, num_queries = query.shape\n",
        "    len_codebook = ref.shape[1]\n",
        "\n",
        "    transfer = torch.empty((len_emb,num_queries), dtype =torch.float)\n",
        "\n",
        "    for q in range(num_queries):\n",
        "      smallest_distance = 1000000000000\n",
        "      nn = 0 # index of nn\n",
        "      query_c = query[:,q] \n",
        "      for c in range(len_codebook):\n",
        "        ref_c = ref[:,c]\n",
        "        delta = query_c - ref_c\n",
        "        distance = torch.sqrt(torch.pow(delta, 2).sum(dim=0))\n",
        "\n",
        "        if distance < smallest_distance:\n",
        "          smallest_distance = distance\n",
        "          nn = c\n",
        "\n",
        "      transfer[:,q] = ref[:,nn]\n",
        "\n",
        "    x_transfered.append(torch.unsqueeze(transfer,0).cuda())\n",
        "      \n",
        "  return x_transfered\n",
        "\n",
        "def transfer_notes(vqvae, instrumet1, instrumet2):\n",
        "  # instrument 1 is tha base\n",
        "  # instrument 2 is the style.   \n",
        "  hx = encode(vqvae, instrumet1)\n",
        "  hy = encode(vqvae, instrumet2)\n",
        "\n",
        "  transfer_enc = transfer_codebook(vqvae, hx, hy)\n",
        "\n",
        "  transfer_enc[0]= transfer_enc[0].float().cuda()\n",
        "  transfer_enc[1]= transfer_enc[1].float().cuda()\n",
        "  transfer_enc[2]= transfer_enc[2].float().cuda()\n",
        "\n",
        "  #instrumet1_dec = decode(vqvae, hx, quant = True)\n",
        "  #instrumet2_dec = decode(vqvae, hy, quant = True)\n",
        "  transfer_dec = decode(vqvae, transfer_enc, False)\n",
        "\n",
        "  return transfer_dec"
      ],
      "metadata": {
        "id": "fhmA1eH2zul7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments\n",
        "In the following cells we run the transfer experiments"
      ],
      "metadata": {
        "id": "aVhYw6it4rh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 1"
      ],
      "metadata": {
        "id": "s2E9SmQ_4xU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "notes_list = ['C4','G4']\n",
        "instruments_list =  ['flute', 'piano', 'trumpet','violin']\n",
        "for note in notes_list:\n",
        "  for i1, i2 in itertools.combinations(instruments_list, 2):\n",
        "    print(i1,i2)\n",
        "    sr, instrumet1 = get_wave(i1, note)\n",
        "    sr, instrumet2 = get_wave(i2, note)\n",
        "    \n",
        "    transfer_dec = transfer_notes(vqvae, instrumet1, instrumet2)\n",
        "    \n",
        "    audio = transfer_dec[0].detach().squeeze().cpu().numpy()\n",
        "    dir = './data/JukeBox Outputs/Single notes/codebook_{i1}-{note}_{i2}-{note}.wav'.format(note = note, i1=i1, i2=i2)\n",
        "    write(dir, sr, audio)\n",
        "\n",
        "    audio = transfer_dec[2].detach().squeeze().cpu().numpy()\n",
        "    dir = './data/JukeBox Outputs/Single notes/codebook-onlylevel2-{i1}-{note}_{i2}-{note}.wav'.format(note = note, i1=i1, i2=i2)\n",
        "    write(dir, sr, audio)\n",
        "\n",
        "    audio = transfer_dec[1].detach().squeeze().cpu().numpy()\n",
        "    dir = './data/JukeBox Outputs/Single notes/codebook-onlylevel1-{i1}-{note}_{i2}-{note}.wav'.format(note = note, i1=i1, i2=i2)\n",
        "    write(dir, sr, audio)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H24vlDzxSyVT",
        "outputId": "dc806e5e-088e-42af-9568-7123a1aca235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flute piano\n",
            "flute trumpet\n",
            "flute violin\n",
            "piano trumpet\n",
            "piano violin\n",
            "trumpet violin\n",
            "flute piano\n",
            "flute trumpet\n",
            "flute violin\n",
            "piano trumpet\n",
            "piano violin\n",
            "trumpet violin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 2"
      ],
      "metadata": {
        "id": "LBBsXOj540ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "song_dir = \"./data/JukeBox Outputs/Moonlight/\"\n",
        "\n",
        "\n",
        "def get_wave(instrumet):\n",
        "  file_name = \"./data/JukeBox Outputs/Moonlight/{}-moonlight.wav\".format(instrumet)\n",
        "  a = read(file_name)\n",
        "  wave = np.mean(a[1][:900000], axis=1)\n",
        "  return a[0], wave\n",
        "\n",
        "\n",
        "instrument_list = ['guitar', 'piano']\n",
        "\n",
        "for i1 in instrument_list:\n",
        "  for i2 in instrument_list:\n",
        "    print(i1,i2)\n",
        "    if i1!=i2:\n",
        "      sr, instrumet1 = get_wave(i1)\n",
        "      sr, instrumet2 = get_wave(i2)\n",
        "\n",
        "      transfer_dec = transfer_notes(vqvae, instrumet1, instrumet2)\n",
        "\n",
        "      audio = transfer_dec[0].detach().squeeze().cpu().numpy()\n",
        "      dir = '{song_dir}/codebook_{i1}-moonlight_{i2}-moonlight.wav'.format(song_dir =song_dir, i1=i1, i2=i2)\n",
        "      write(dir, sr, audio)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x59ycnl_lhAZ",
        "outputId": "dee5decd-fb3d-41c7-86ca-9a734405945d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "guitar guitar\n",
            "guitar piano\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 3"
      ],
      "metadata": {
        "id": "5tth42-I4-KY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "banjo, banjo_sr = librosa.load(\"./data/JukeBox Outputs/SS VQ-VAE dataset/banjo.mp3\")\n",
        "church, church_sr = librosa.load(\"./data/JukeBox Outputs/SS VQ-VAE dataset/church-organ.mp3\")\n",
        "\n",
        "banjo = banjo[:150000]\n",
        "church = church[:150000]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw_LpLe6xZc6",
        "outputId": "487bb503-9b97-46d6-df98-797deca7667e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transfer_dec = transfer_notes(vqvae, banjo, church)\n",
        "\n",
        "audio = transfer_dec[0].detach().squeeze().cpu().numpy()\n",
        "dir = './data/JukeBox Outputs/SS VQ-VAE dataset/codebook_banjo_church-organ.wav'\n",
        "write(dir, banjo_sr, audio)\n"
      ],
      "metadata": {
        "id": "7JeTnyflhq1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transfer_dec = transfer_notes(vqvae, church, banjo)\n",
        "\n",
        "audio = transfer_dec[0].detach().squeeze().cpu().numpy()\n",
        "dir = './data/JukeBox Outputs/SS VQ-VAE dataset/codebook_church-organ_banjo.wav'\n",
        "write(dir, church_sr, audio)\n"
      ],
      "metadata": {
        "id": "niwuDIt8xeI3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}