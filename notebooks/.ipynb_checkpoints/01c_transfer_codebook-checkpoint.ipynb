{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43CgTMdpjJ2-"
   },
   "source": [
    "\n",
    "# Style Transfer: Codebook\n",
    "\n",
    "In this notebook we describe and demostrate our simplest method that was used as a baseline model. Recall that the encoder first transforms the input audio into latent vectors $h_s$ and then the vectors are quantized to the closest codebook vector $e_s$. The intuition behind this first method is to quantize the latent vectors from $x$ (content) using only the unique vectors from $y$ (style).\n",
    "\n",
    "First, we obtain the latent vectors from $x$ and $y$ ( $h_x$ and $h_y$ ). For $y$ we quantize the vectors using the full codebook to obtain $e_y$ and for $x$ we quantize the vectors using only the unique vectors from $e_y$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "taDHgk1WCC_C",
    "outputId": "d51a47a4-4def-4943-8f48-101a558409a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda True\n"
     ]
    }
   ],
   "source": [
    "import jukebox\n",
    "from torch import float64\n",
    "import torch as t\n",
    "import torch\n",
    "import librosa\n",
    "import os\n",
    "from IPython.display import Audio\n",
    "from jukebox.make_models import make_vqvae, make_prior, MODELS, make_model\n",
    "from jukebox.hparams import Hyperparams, setup_hparams\n",
    "from jukebox.sample import sample_single_window, _sample, \\\n",
    "                           sample_partial_window, upsample\n",
    "from jukebox.utils.dist_utils import setup_dist_from_mpi\n",
    "from jukebox.utils.torch_utils import empty_cache\n",
    "rank, local_rank, device = setup_dist_from_mpi()\n",
    "\n",
    "import scipy\n",
    "import torch\n",
    "import numpy as np\n",
    "import lucent\n",
    "from lucent.optvis.render import hook_model\n",
    "from lucent.modelzoo.util import get_model_layers\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.wavfile import read, write\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65aR2OZxmfzq",
    "outputId": "ecb72a03-3c9e-4efa-bdac-fa214d45d37e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from azure\n",
      "Running  wget -O /root/.cache/jukebox/models/5b/vqvae.pth.tar https://openaipublic.azureedge.net/jukebox/models/5b/vqvae.pth.tar\n",
      "Restored from /root/.cache/jukebox/models/5b/vqvae.pth.tar\n",
      "0: Loading vqvae in eval mode\n",
      "Creating cond. autoregress with prior bins [79, 2048], \n",
      "dims [384, 6144], \n",
      "shift [ 0 79]\n",
      "input shape 6528\n",
      "input bins 2127\n",
      "Self copy is False\n",
      "Loading artist IDs from /usr/local/lib/python3.7/dist-packages/jukebox/data/ids/v3_artist_ids.txt\n",
      "Loading artist IDs from /usr/local/lib/python3.7/dist-packages/jukebox/data/ids/v3_genre_ids.txt\n",
      "Level:2, Cond downsample:None, Raw to tokens:128, Sample length:786432\n",
      "Downloading from azure\n",
      "Running  wget -O /root/.cache/jukebox/models/1b_lyrics/prior_level_2.pth.tar https://openaipublic.azureedge.net/jukebox/models/1b_lyrics/prior_level_2.pth.tar\n",
      "Restored from /root/.cache/jukebox/models/1b_lyrics/prior_level_2.pth.tar\n",
      "0: Loading prior in eval mode\n"
     ]
    }
   ],
   "source": [
    "model = \"1b_lyrics\" # or \"1b_lyrics\"     \n",
    "hps = Hyperparams()\n",
    "hps.sr = 44100\n",
    "hps.n_samples = 3 if model=='5b_lyrics' else 8\n",
    "hps.name = 'samples'\n",
    "chunk_size = 16 if model==\"5b_lyrics\" else 32\n",
    "max_batch_size = 3 if model==\"5b_lyrics\" else 16\n",
    "hps.levels = 3\n",
    "hps.hop_fraction = [.5,.5,.125]\n",
    "\n",
    "vqvae, *priors = MODELS[model]\n",
    "vqvae = make_vqvae(setup_hparams(vqvae, dict(sample_length = 1048576)), device)\n",
    "top_prior = make_prior(setup_hparams(priors[-1], dict()), vqvae, device)\n",
    "vqvae = vqvae.eval()\n",
    "\n",
    "f_start = 100\n",
    "f_end = 4000\n",
    "num_seconds = 10\n",
    "sample_rate = 44100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhmA1eH2zul7"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def encode(vqvae, wave):\n",
    "  x = torch.from_numpy(wave).unsqueeze(0).unsqueeze(2).cuda()\n",
    "  x_in = vqvae.preprocess(x)\n",
    "  xs = []\n",
    "  for level in range(vqvae.levels):\n",
    "      encoder = vqvae.encoders[level]\n",
    "      x_out = encoder(x_in)\n",
    "      xs.append(x_out[-1])\n",
    "\n",
    "  return xs\n",
    "\n",
    "def decode(vqvae, xs, quant=True):\n",
    "\n",
    "  if quant:\n",
    "    zs, xs_quantised, _, _ = vqvae.bottleneck(xs)\n",
    "  else:\n",
    "    xs_quantised = xs \n",
    "\n",
    "  x_outs = []\n",
    "  x_outs_nonquantised = []\n",
    "  for level in range(vqvae.levels):\n",
    "      decoder = vqvae.decoders[level]    \n",
    "      x_out = decoder(xs_quantised[level:level+1], all_levels=False)\n",
    "      x_outs.append(x_out)\n",
    "      x_outs[level] = vqvae.postprocess(x_outs[level])\n",
    "\n",
    "  return x_outs\n",
    "\n",
    "\n",
    "def make_continous(discrete, continous):\n",
    "  a, num_discrete = discrete.shape\n",
    "  _, num_continous = continous.shape\n",
    "  ref = torch.empty((a,num_discrete), dtype =torch.float)\n",
    "\n",
    "  for d in range(num_discrete):\n",
    "    smallest_distance = 1000000000000\n",
    "    nn = 0 # index of nn\n",
    "    current_discrete = discrete[:,d]\n",
    "\n",
    "    for c in range(num_continous):\n",
    "      current_cont = continous[:,c]\n",
    "      delta = current_discrete - current_cont \n",
    "      distance = torch.sqrt(torch.pow(delta, 2).sum(dim=0))\n",
    "\n",
    "      if distance < smallest_distance:\n",
    "        smallest_distance = distance\n",
    "        nn = c\n",
    "\n",
    "    ref[:,d] = continous[:,nn]\n",
    "\n",
    "  return ref.cuda()\n",
    "\n",
    "\n",
    "def transfer_codebook(vqvae, hx, hy):\n",
    "  # Content\n",
    "  num_layers = len(hx)\n",
    "\n",
    "  # Style\n",
    "  zs, y_quantised, _, _ = vqvae.bottleneck(hy)\n",
    "\n",
    "  x_transfered = []\n",
    "\n",
    "  for layer in range(num_layers):\n",
    "    query = hx[layer][0]\n",
    "\n",
    "    ref_discrete = torch.unique(torch.floor(y_quantised[layer][0]), dim=1)\n",
    "    ref = make_continous(ref_discrete, y_quantised[layer][0])\n",
    "\n",
    "    len_emb, num_queries = query.shape\n",
    "    len_codebook = ref.shape[1]\n",
    "\n",
    "    transfer = torch.empty((len_emb,num_queries), dtype =torch.float)\n",
    "\n",
    "    for q in range(num_queries):\n",
    "      smallest_distance = 1000000000000\n",
    "      nn = 0 # index of nn\n",
    "      query_c = query[:,q] \n",
    "      for c in range(len_codebook):\n",
    "        ref_c = ref[:,c]\n",
    "        delta = query_c - ref_c\n",
    "        distance = torch.sqrt(torch.pow(delta, 2).sum(dim=0))\n",
    "\n",
    "        if distance < smallest_distance:\n",
    "          smallest_distance = distance\n",
    "          nn = c\n",
    "\n",
    "      transfer[:,q] = ref[:,nn]\n",
    "\n",
    "    x_transfered.append(torch.unsqueeze(transfer,0).cuda())\n",
    "      \n",
    "  return x_transfered\n",
    "\n",
    "def transfer_notes(vqvae, instrumet1, instrumet2):\n",
    "  # instrument 1 is tha base\n",
    "  # instrument 2 is the style.   \n",
    "  hx = encode(vqvae, instrumet1)\n",
    "  hy = encode(vqvae, instrumet2)\n",
    "\n",
    "  transfer_enc = transfer_codebook(vqvae, hx, hy)\n",
    "\n",
    "  transfer_enc[0]= transfer_enc[0].float().cuda()\n",
    "  transfer_enc[1]= transfer_enc[1].float().cuda()\n",
    "  transfer_enc[2]= transfer_enc[2].float().cuda()\n",
    "\n",
    "  #instrumet1_dec = decode(vqvae, hx, quant = True)\n",
    "  #instrumet2_dec = decode(vqvae, hy, quant = True)\n",
    "  transfer_dec = decode(vqvae, transfer_enc, False)\n",
    "\n",
    "  return transfer_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVhYw6it4rh6"
   },
   "source": [
    "# Experiments\n",
    "In the following cells we run the transfer experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2E9SmQ_4xU8"
   },
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H24vlDzxSyVT",
    "outputId": "dc806e5e-088e-42af-9568-7123a1aca235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flute piano\n",
      "flute trumpet\n",
      "flute violin\n",
      "piano trumpet\n",
      "piano violin\n",
      "trumpet violin\n",
      "flute piano\n",
      "flute trumpet\n",
      "flute violin\n",
      "piano trumpet\n",
      "piano violin\n",
      "trumpet violin\n"
     ]
    }
   ],
   "source": [
    "data_dir = './../data/exp1/'\n",
    "output_dir =  './../results/exp1/'\n",
    "\n",
    "\n",
    "def get_wave(instrument, note):    \n",
    "  file_name = f\"{data_dir}/{instrument}-{note}.wav\"\n",
    "  wave = np.array(a[1],dtype='int16')\n",
    "  return a[0], wave\n",
    "\n",
    "notes_list = ['C4','G4']\n",
    "instruments_list =  ['flute', 'piano', 'trumpet','violin', 'sine']\n",
    "for note in notes_list:\n",
    "  for i1, i2 in itertools.permutations(instruments_list, 2):\n",
    "    print(i1,i2)\n",
    "    sr, instrumet1 = get_wave(i1, note)\n",
    "    sr, instrumet2 = get_wave(i2, note)\n",
    "    \n",
    "    transfer_dec = transfer_notes(vqvae, instrumet1, instrumet2)\n",
    "    \n",
    "    audio = transfer_dec[0].detach().squeeze().cpu().numpy()\n",
    "    dir = f'{output_dir}/codebook_{i1}-{note}_{i2}-{note}.wav'\n",
    "    write(dir, sr, audio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBBsXOj540ev"
   },
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x59ycnl_lhAZ",
    "outputId": "dee5decd-fb3d-41c7-86ca-9a734405945d"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guitar guitar\n",
      "guitar piano\n"
     ]
    }
   ],
   "source": [
    "data_dir = './../data/exp2/'\n",
    "output_dir =  './../results/exp2/'\n",
    "\n",
    "def get_wave(instrumet):\n",
    "  file_name = f\"{data_dir}/{instrument}-moonlight.wav\"\n",
    "  a = read(file_name)\n",
    "  wave = np.mean(a[1][:900000], axis=1)\n",
    "  return a[0], wave\n",
    "\n",
    "\n",
    "instrument_list = ['guitar', 'piano']\n",
    "\n",
    "for i1 in instrument_list:\n",
    "  for i2 in instrument_list:\n",
    "    print(i1,i2)\n",
    "    if i1!=i2:\n",
    "      sr, instrumet1 = get_wave(i1)\n",
    "      sr, instrumet2 = get_wave(i2)\n",
    "\n",
    "      transfer_dec = transfer_notes(vqvae, instrumet1, instrumet2)\n",
    "\n",
    "      audio = transfer_dec[0].detach().squeeze().cpu().numpy()\n",
    "      dir = f'{output_dir}/codebook_{i1}-moonlight_{i2}-moonlight.wav'\n",
    "      write(dir, sr, audio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tth42-I4-KY"
   },
   "source": [
    "## Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tw_LpLe6xZc6",
    "outputId": "487bb503-9b97-46d6-df98-797deca7667e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "data_dir = './../data/exp3/'\n",
    "output_dir =  './../results/exp3/'\n",
    "\n",
    "banjo, banjo_sr = librosa.load(f\"{data_dir}/banjo.mp3\")\n",
    "church, church_sr = librosa.load(f\"{data_dir}/church-organ.mp3\")\n",
    "\n",
    "banjo = banjo[:150000]\n",
    "church = church[:150000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JeTnyflhq1r"
   },
   "outputs": [],
   "source": [
    "transfer_dec = transfer_notes(vqvae, banjo, church)\n",
    "\n",
    "audio = transfer_dec[0].detach().squeeze().cpu().numpy()\n",
    "dir = f\"{output_dir}/codebook_banjo_church-organ.wav\"\n",
    "write(dir, banjo_sr, audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niwuDIt8xeI3"
   },
   "outputs": [],
   "source": [
    "transfer_dec = transfer_notes(vqvae, church, banjo)\n",
    "\n",
    "audio = transfer_dec[0].detach().squeeze().cpu().numpy()\n",
    "dir = f\"{output_dir}/codebook_church-organ_banjo.wav\"\n",
    "write(dir, church_sr, audio)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "01-transfer-codebook.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
