{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51536ab2",
   "metadata": {},
   "source": [
    "# Style Transfer: Method Adain and WCT\n",
    "\n",
    "In this notebook we describe and demostrate the style transfer with the Adain method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca3a921-a7d1-47cc-8099-7eee72f0e311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda True\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import jukebox\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import scipy\n",
    "import os\n",
    "import numpy as np\n",
    "import nussl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Audio\n",
    "from jukebox.make_models import make_vqvae, make_prior, MODELS, make_model\n",
    "from jukebox.hparams import Hyperparams, setup_hparams\n",
    "from jukebox.utils.dist_utils import setup_dist_from_mpi\n",
    "from jukebox.utils.torch_utils import empty_cache\n",
    "from jukebox.utils.jukebox_utils import get_forward_calls_encoder, split_model\n",
    "rank, local_rank, device = setup_dist_from_mpi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed067c40-4f87-4179-81d1-3d7130e77713",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from azure\n",
      "Restored from /home/ozaydin/.cache/jukebox/models/5b/vqvae.pth.tar\n",
      "0: Loading vqvae in eval mode\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = \"5b\" # or \"1b_lyrics\"     \n",
    "vqvae, *priors = MODELS[model]\n",
    "hparams = setup_hparams(vqvae, dict(sample_length = 1048576))\n",
    "vqvae = make_vqvae(hparams, device)\n",
    "vqvae = vqvae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c910d3-cf92-4232-8da3-3d15bff0605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seconds = 20\n",
    "sample_rate = 44100\n",
    "t = np.linspace(0, num_seconds, sample_rate * num_seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1236cc-689f-4be1-8560-250495d685fc",
   "metadata": {},
   "source": [
    "## Access Hidden Layers of JukeBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9593ed3-98e8-4d84-9182-670ae4778713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose the level of JukeBox and hook it\n",
    "from lucent.optvis.render import hook_model\n",
    "from functools import partial\n",
    "from jukebox.utils.jukebox_utils import get_forward_calls_encoder, get_forward_calls_decoder, split_model, compose_funclist\n",
    "\n",
    "level = 2\n",
    "encoder = vqvae.encoders[level]\n",
    "bottleneck = vqvae.bottleneck.level_blocks[level]\n",
    "decoder = vqvae.decoders[level]\n",
    "\n",
    "encoder_hook, encoder_layers = hook_model(encoder, include_class_name=False)\n",
    "encoder_calls, encoder_layer_names = get_forward_calls_encoder(encoder, prefix=\"\")\n",
    "decoder_hook, decoder_layers = hook_model(decoder, include_class_name=False)\n",
    "decoder_calls, decoder_layer_names = get_forward_calls_decoder(decoder, prefix=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8bc321-3a70-4a1a-8351-344d798fc7d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split Encoder and Decoder\n",
    "enc_layer_index = -3\n",
    "l_enc = encoder_layer_names[enc_layer_index]\n",
    "dec_layer_index = -3\n",
    "l_dec = decoder_layer_names[dec_layer_index]\n",
    "\n",
    "pre_enc, post_enc = split_model(encoder, l_enc, partial(get_forward_calls_encoder, prefix=\"\"))\n",
    "pre_dec, post_dec = split_model(decoder, l_dec, partial(get_forward_calls_decoder, prefix=\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f29d625e-5372-4ca0-9f0f-e505d044d787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jukebox.utils.jukebox_utils import slice_model\n",
    "# Slice Encoder and Decoder\n",
    "enc_layer_indices = [1,3,5,8]\n",
    "enc_layer_names = [encoder_layer_names[layer_index] for layer_index in enc_layer_indices]\n",
    "dec_layer_indices = [2,5,]\n",
    "dec_layer_names = [decoder_layer_names[layer_index] for layer_index in dec_layer_indices]\n",
    "\n",
    "enc_slices = slice_model(encoder, enc_layer_names, partial(get_forward_calls_encoder, prefix=\"\"))\n",
    "dec_slices = slice_model(decoder, dec_layer_names, partial(get_forward_calls_decoder, prefix=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d601bc68-41bd-43d0-92b7-08e328811b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Encoder and Decoder\n",
    "pre = compose_funclist([\n",
    "    pre_enc,\n",
    "    post_enc,\n",
    "    lambda x: bottleneck(x)[1],\n",
    "])\n",
    "\n",
    "post = compose_funclist([\n",
    "    #lambda x: bottleneck(x)[1],\n",
    "    pre_dec,\n",
    "    post_dec,\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f99231-a206-44c4-bb52-e3a43220f948",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions to play and save audio\n",
    "from jukebox.utils.jukebox_utils import compose_funclist\n",
    "play = compose_funclist([\n",
    "    lambda x: x.detach().cpu().numpy()[0,0],\n",
    "    lambda x: x / max(x.max(), -x.min()),\n",
    "    lambda x: nussl.AudioSignal(audio_data_array=x, sample_rate=44100),\n",
    "    lambda x: x.embed_audio()\n",
    "])\n",
    "\n",
    "post_process = compose_funclist([\n",
    "    lambda x: x.detach().cpu().numpy()[0,0],\n",
    "    lambda x: x / max(x.max(), -x.min()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2698a4-fab0-4175-a777-9594974ccfcf",
   "metadata": {},
   "source": [
    "# Style Transfer Functions\n",
    "\n",
    "Given source and target audio signals $x, y \\in \\mathbb{R}^{1 \\times T}$ where $T$ is temporal length of the signal, our aim is to generate a translation $x \\rightarrow y$ that preserves the 'content' of $x$ and adopts the 'style' from $y$.\n",
    "\n",
    "To that end, we first extract deep features of $x$ and $y$ from a state-of-the-art autoencoder trained on musical signals, JukeBox. We split JukeBox into two parts $F_{pre}: \\mathbb{R}^{1 \\times T} \\rightarrow\\mathbb{R}^{C \\times T'}$ and $F_{post}: \\mathbb{R}^{C \\times T'} \\rightarrow \\mathbb{R}^{1 \\times T}$ where $C$ denotes the number of feature channels and $T'$ is the temporal dimension of the deep features. The features $h_x, h_y \\in \\mathbb{R}^{C \\times T'}$ are formally computed as \n",
    "\n",
    "$$\n",
    "h_s = F_{pre}(s)\n",
    "$$\n",
    "\n",
    "where $s \\in \\{x,y\\}$ is the input signal to JukeBox.\n",
    "\n",
    "Extracted features $h_s$ can be directly decoded to reconstruct the input audio signal $\\hat s \\sim F_{post}(F_{pre}(s))$. In our work, we propose a transforming the deep features in order to perform style transfer. We denote our transformation function as $F_{trans}: (\\mathbb{R}^{C \\times T'}, \\mathbb{R}^{C \\times T'}) \\rightarrow \\mathbb{R}^{C \\times T'} $ and the transformed features as $h_{x \\rightarrow y}$.\n",
    "\n",
    "$$\n",
    "h_{x \\rightarrow y} = F_{trans}(h_x, h_y)\\\\\n",
    "$$\n",
    "\n",
    "Finally, we obtain the translation as\n",
    "\n",
    "$$\n",
    "x \\rightarrow y = F_{post}(h_{x \\rightarrow y})\n",
    "$$\n",
    "\n",
    "In the next sections, we describe different transformation functions we used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f75be18-e693-4329-bec9-267f5f9baffc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Adaptive Instance Normalziation (AdaIN)\n",
    "\n",
    "We first use AdaIN to implement $F_{trans}$ as $F_{AdaIN}$.\n",
    "\n",
    "$$\n",
    "F_{AdaIN}(h_x, h_y) = \\sigma_{h_y} \\frac{h_x - \\mu_{h_x}}{\\sigma_{h_x}} + \\mu_y\n",
    "$$\n",
    "\n",
    "where $\\mu_{h_s}, \\sigma_{h_s} \\in \\mathbb{R}^{C \\times 1}$  temporal average and standard deviation of $h_s$ respectively.\n",
    "\n",
    "The translation using AdaIN is defined as\n",
    "$$\n",
    "\\begin{align}\n",
    "x \\xrightarrow{AdaIN} y &= F_{post}(F_{AdaIN}(F_{pre}(x), F_{pre}(y)))\\\\\n",
    "                        &= F_{post}(F_{AdaIN}(h_x, h_y))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee721685-1cc7-4a84-b3e7-7d81f0ad8f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaIN functions\n",
    "def compute_stats(x):\n",
    "    mean = x.mean(dim=-1).contiguous()[...,None]\n",
    "    std = x.std(dim=-1).contiguous()[...,None]\n",
    "    return mean, std\n",
    "\n",
    "def normalize(x, eps=1e-10):\n",
    "    normalize_mean, normalize_std = compute_stats(x)\n",
    "    x_normalized = (x - normalize_mean)/(normalize_std+eps)\n",
    "    #out = F.batch_norm(a, None, None, training=True)\n",
    "    return  x_normalized\n",
    "\n",
    "def modulate(x, mean, std):\n",
    "    x_modulated = x * std + mean\n",
    "    return  x_modulated\n",
    "\n",
    "def AdaIN(x, mean, std):\n",
    "    x_normalized = normalize(x)\n",
    "    x_modulated = modulate(x_normalized, mean, std)\n",
    "    return x_modulated\n",
    "\n",
    "# AdaIN using torch\n",
    "def AdaIN_torch(x, mean, std):\n",
    "    return torch.nn.functional.batch_norm(x, None, None, training=True, weight=std, bias=mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5712b4-8101-4b3c-9768-0d81d9b6e49e",
   "metadata": {},
   "source": [
    "## Whitening and Coloring Transform (WCT)\n",
    "We then use WCT to implement $F_{trans}$ as $F_{WCT}$. We first extract the normalized features  as $\\hat{h}_s = h_s - \\mu_{h_s}$. Then we apply eigen-decomposition to the normalized features \n",
    "\n",
    "$$\n",
    "\\frac{1}{T'} \\hat{h}_s \\hat{h}_s^T = E_s A_s E_s^T\n",
    "$$\n",
    "\n",
    "We perform the whitening on the source features $h_x$ to obtain an uncorrelated feature map.\n",
    "\n",
    "$$\n",
    "h_x^{white} = E_x A_x^{-1/2} E_x^T \\hat{h}_x\n",
    "$$\n",
    "\n",
    "We then colorize $h_x^{white}$ with $\\hat{h}_y$\n",
    "\n",
    "$$\n",
    "h_x^{colored_y} = E_y A_y^{1/2} E_y^T h_x^{white}\n",
    "$$\n",
    "\n",
    "Finally, we add the mean of $h_y$ to the colored features\n",
    "\n",
    "$$\n",
    "h_{x \\rightarrow y} = h_x^{colored_y} + \\mu_y\n",
    "$$\n",
    "\n",
    "The transformation function for WCT then becomes\n",
    "\n",
    "$$\n",
    "F_{WCT}(h_x, h_y) = E_y A_y^{1/2} E_y^T (E_x A_x^{-1/2} E_x^T (h_x - \\mu_{h_x})) + \\mu_{h_y}\n",
    "$$\n",
    "\n",
    "Notice that similar to AdaIN, we normalized the features along the temporal dimension. However, WCT applies whitening and coloring transforms instead of the standardization used in AdaIN. In that sense, WCT is a stronger transformation compared to AdaIN. Applying stronger transforms is advantageous as it brings the feature statistics of the target and translation signals closer to each other. On the other hand, stronger test time transformations create discrepancy between testing and training, which might result in less realistic outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dadb0a25-a995-4d7e-afc8-74277b5b6bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WCT functions\n",
    "def get_gram_matrix(h, remove_mean=True):\n",
    "    b, c, T = h.shape\n",
    "    assert b == 1\n",
    "    f = h[0]\n",
    "    if remove_mean:\n",
    "        f = f - f.mean(dim=1, keepdim=True)\n",
    "    G = torch.matmul(f, f.permute(1, 0))\n",
    "    G = G / T\n",
    "    \n",
    "    return G\n",
    "\n",
    "def whitening_transform(h):\n",
    "    eps = 1e-10\n",
    "    b, c, T = h.shape\n",
    "    assert b == 1\n",
    "    \n",
    "    h = h.to(torch.double)\n",
    "    G = get_gram_matrix(h).to(torch.double)\n",
    "    \n",
    "    \n",
    "    E, D, V = torch.linalg.svd(G, full_matrices=True)\n",
    "    \n",
    "    i = len(D)\n",
    "    for j in range(len(D)):\n",
    "        if D[j] < eps:\n",
    "            i = j\n",
    "            break\n",
    "            \n",
    "    \n",
    "    Dt = D[:i]    \n",
    "    Dt = Dt ** (-0.5)\n",
    "    Dt = torch.diag(Dt)\n",
    "    D = torch.diag(D)\n",
    "    Et = E[:, :i]\n",
    "    Vt = V[:i, :]\n",
    "    A = Et @ Dt @ Et.T\n",
    "    \n",
    "    f = h.reshape(c, T)\n",
    "    f = f - f.mean(dim=1, keepdim=True)\n",
    "    wh = A @ f\n",
    "\n",
    "        \n",
    "    wh = wh.reshape(b, c, T)\n",
    "    return wh\n",
    "\n",
    "def coloring_transform(wh, h_style):\n",
    "    b, c, T = h_style.shape\n",
    "    assert b == 1\n",
    "        \n",
    "    h_style = h_style.to(torch.double)\n",
    "    G_style = get_gram_matrix(h_style).to(torch.double)\n",
    "    \n",
    "    E, D, V = torch.linalg.svd(G_style, full_matrices=True)\n",
    "\n",
    "    D = D ** (0.5)\n",
    "    D = torch.diag(D)\n",
    "    A = E @ D\n",
    "    A = A @ E.T\n",
    "    A = A\n",
    "    \n",
    "    b2, c2, T2 = wh.shape\n",
    "    wh = wh.reshape(c2, T2)\n",
    "    # wh = wh\n",
    "    sh = A @ wh\n",
    "    sh = sh + h_style.reshape(c, T).mean(dim=1, keepdim=True)\n",
    "    sh = sh.reshape(b2, c2, T2)\n",
    "    \n",
    "    return sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13177005-50a5-4eac-952f-35ccfec90541",
   "metadata": {},
   "source": [
    "# Applying Style Transfer Functions in Hidden Layers\n",
    "In order to have a stronger control over the style, we propose to use our transformation functions in multiple feature layers of JukeBox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22fa260b-fd83-4a6e-b862-5a01ea60f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features form a sliced model\n",
    "def sliced_features(x, slices):\n",
    "    feats = []\n",
    "    stats = []\n",
    "    for block in slices:\n",
    "        stat = compute_stats(x) \n",
    "        feats.append(x)\n",
    "        stats.append(stat)\n",
    "        x = block(x)\n",
    "    feats.append(x)\n",
    "    return stats, feats\n",
    "\n",
    "# AdaIN for a sliced model\n",
    "def sliced_AdaIN(x, y_stats, slices):\n",
    "    assert len(y_stats) == len(slices)\n",
    "    feats = []\n",
    "    for stat, block in zip(y_stats, slices):\n",
    "        x = AdaIN(x, *stat)\n",
    "        feats.append(x)\n",
    "        x = block(x)\n",
    "    feats.append(x)\n",
    "    return feats\n",
    "\n",
    "# WCT for a sliced model\n",
    "def sliced_WCT(x, y_feats, slices):\n",
    "    assert len(y_feats) == len(slices) + 1, 'y: {}, slices: {}'.format(len(y_feats), len(slices))\n",
    "    feats = []\n",
    "    for y, block in zip(y_feats, slices):\n",
    "        x_w = whitening_transform(x)\n",
    "        x_wct = coloring_transform(x_w, y).to(torch.float32)\n",
    "        feats.append(x_wct)\n",
    "        x = block(x_wct)\n",
    "    feats.append(x)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abf633e6-89c5-48f0-8c6b-1a22ffa246fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transfer(x_content, x_style, method='adain'):\n",
    "    assert method in ['adain', 'adainmulti', 'wct', 'wctmulti']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hc = pre(x_content)\n",
    "        hs = pre(x_style)\n",
    "        \n",
    "        if method == 'adain':\n",
    "            hs_stats = compute_stats(hs)\n",
    "            h_transformed = AdaIN(hc, *hs_stats)\n",
    "            x_transformed = post(h_transformed)\n",
    "        elif method == 'adainmulti':\n",
    "            hs_stats, hs_feats = sliced_features(hs, dec_slices)\n",
    "            h_transformed = sliced_AdaIN(hc, hs_stats, dec_slices)\n",
    "            x_transformed = h_transformed[-1]\n",
    "            \n",
    "        elif method == 'wct':\n",
    "            h_white = whitening_transform(hc)\n",
    "            h_transformed = coloring_transform(h_white, hs).to(torch.float32)\n",
    "            x_transformed = post(h_transformed)\n",
    "        \n",
    "        elif method == 'wctmulti':\n",
    "            hs_stats, hs_feats = sliced_features(hs, dec_slices)\n",
    "            h_transformed = sliced_WCT(hc, hs_feats, dec_slices)\n",
    "            x_transformed = h_transformed[-1]\n",
    "            \n",
    "        return x_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2989a7-e73a-4454-820f-9a27e51366d1",
   "metadata": {},
   "source": [
    "## Loading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4f8c4b8-320d-4c01-b265-f8a0699cabad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wave wiles\n",
    "\n",
    "data_dir = './../data/exp1/'\n",
    "output_dir =  './../results/exp1/'\n",
    "\n",
    "t = np.linspace(0, num_seconds, num_seconds * sample_rate)\n",
    "freq_C4 = 261.6\n",
    "\n",
    "names_list = ['trumpet-C4', 'flute-C4', 'piano-C4', 'violin-C4', ]\n",
    "waves_list = [librosa.load('{}{}.wav'.format(data_dir, name), sr=sample_rate)[0][:num_seconds * sample_rate] for name in names_list]\n",
    "\n",
    "names_list.append('sine-C4')\n",
    "waves_list.append(np.sin(2 * np.pi * freq_C4 * t))\n",
    "\n",
    "names_list.append('sawtooth-C4')\n",
    "waves_list.append(scipy.signal.sawtooth(2 * np.pi * freq_C4 * t))\n",
    "\n",
    "names_list.append('square-C4')\n",
    "waves_list.append(scipy.signal.square(2 * np.pi * freq_C4 * t))\n",
    "\n",
    "tensors_list = [torch.Tensor(wav).to(device)[None,None,:] / max(wav.max(), -wav.min()) for wav in waves_list]\n",
    "x_dict = {name: tensor for name,tensor in zip(names_list, tensors_list)}\n",
    "\n",
    "methods = ['adain', 'adainmulti', 'wct', 'wctmulti']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ff6cf76-47d5-4f07-9db9-9ba907b4e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in methods:\n",
    "    for x_name, x in x_dict.items():\n",
    "        sf.write('{}/{}.wav'.format(output_dir, x_name), post_process(x), sample_rate)\n",
    "        for y_name, y in x_dict.items():\n",
    "            x_transformed = style_transfer(x, y, method=method)\n",
    "            #play(x_transformed)\n",
    "            sf.write('{}/{}_{}_{}.wav'.format(output_dir, method, x_name, y_name), post_process(x_transformed), sample_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
